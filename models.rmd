```{r}
library(MASS)
library(tidyverse)
library(boot)
library(caret)
library(MLmetrics)
library(glmnet)
library(car)
```
some notes
- highly imbalanced class - use f1 score instead
- 

to do:
- actually clean the data lol - the classes are fucked for country and
duplicate nike in brands

```{r}
hist_mar <- read.csv('historical_marathon_dataset.csv')
hist_mar$gender <- as.factor(hist_mar$gender)

# country has lots of levels for US and typo for Australia
hist_mar$country[hist_mar$country == "Aus"] <- "Australia"
hist_mar$country[hist_mar$country == "Australaia"] <- "Australia"
hist_mar$country[hist_mar$country == "US"] <- "USA"
hist_mar$country[hist_mar$country == "United States"] <- "USA"
hist_mar$country[is.na(hist_mar$country)] <- "Unkown"
hist_mar$country <- as.factor(hist_mar$country)

# shoe brand has some typos, and encode NA's as a new class
hist_mar$shoe_brand[hist_mar$shoe_brand == "Addas"] <- "Adidas"
hist_mar$shoe_brand[is.na(hist_mar$shoe_brand)] <- "Unkown"
hist_mar <- hist_mar %>%
  mutate(shoe_brand = str_trim(shoe_brand))
hist_mar$shoe_brand <- as.factor(hist_mar$shoe_brand)

# remove negative observations in weekly_km
hist_mar <- hist_mar[hist_mar$weekly_km >= 0, ]

# properly encode boolean features
hist_mar <- hist_mar %>%
  mutate(across(c(injured_prev_mth, injured_prev_qtr, injured_prev_hy), as.logical))

event_summary <- read.csv('event_summary.csv')
event_summary <- event_summary %>%
  mutate(across(c(gel_support, stretching_station, music_at_start), as.logical))

data <- hist_mar %>%
  left_join(event_summary, by = "year")

data$needed_med <- !is.na(data$medical_km_bin)
hist_mar$needed_med <- !is.na(hist_mar$medical_km_bin)
```


```{r}
model <- lm(needed_med ~.-year-medical_km_bin, data=data)
vif(model)
```










# LASSO CV f1

pick the dataset lol this is so cooked
```{r}
class_data <- data.frame(data)
class_data$medical_km_bin <- NULL
class_data$finish_time <- NULL
class_data <- na.omit(class_data)

# Create engineered features
class_data_engineered <- class_data %>%
  mutate(
    # === PHYSICAL PERFORMANCE FEATURES ===
    bmi = weight / (height/100)^2,
    training_intensity = weekly_km / age,
    experience_per_age = marathons_xp / age,
    
    # === INJURY HISTORY PATTERNS ===
    injury_cascade = as.numeric(injured_prev_mth == 1 & injured_prev_qtr == 1),
    chronic_injury_pattern = as.numeric(injured_prev_mth == 1 & injured_prev_qtr == 1 & injured_prev_hy == 1),
    recent_only_injury = as.numeric(injured_prev_mth == 1 & injured_prev_qtr == 0 & injured_prev_hy == 0),
    worsening_injury = as.numeric(injured_prev_mth == 1 & injured_prev_qtr == 0),
    improving_injury = as.numeric(injured_prev_mth == 0 & injured_prev_qtr == 1),
    injury_free_recent = as.numeric(injured_prev_mth == 0 & injured_prev_qtr == 0),
    
    # === ENVIRONMENTAL STRESS FACTORS ===
    heat_humidity_interaction = temp_10am * humidity / 100,
    temp_humidity_ratio = ifelse(humidity > 0, temp_10am / humidity, temp_10am),
    extreme_heat = as.numeric(temp_10am > 26 & humidity > 71), #using 3rd quantile
    extreme_weather = as.numeric(temp_10am > 30 | humidity > 80 | rainfall > 3.8), # 3rd quantile again
    high_temp_low_humidity = as.numeric(temp_10am > 26 & humidity < 58),
    
    # === RACE SUPPORT QUALITY ===
    hydration_per_crowd = hydration_stations / crowding_density,
    toilet_per_crowd = toilet_stations / crowding_density,
    has_performance_support = as.numeric(gel_support == 1 | stretching_station == 1),
    full_support_package = as.numeric(gel_support == 1 & stretching_station == 1),
    minimal_hydration = as.numeric(hydration_stations <= 2),
    inadequate_facilities = as.numeric(toilet_stations <= 1),
    high_crowd_low_hydration = as.numeric(crowding_density > median(crowding_density, na.rm = TRUE) & 
                                         hydration_stations < median(hydration_stations, na.rm = TRUE)),
    support_mismatch = as.numeric((hydration_stations < 3) & (gel_support == 1)),
    
    # === DEMOGRAPHIC INTERACTIONS ===
    age_experience_mismatch = age - marathons_xp * 2,
    
    # === CATEGORICAL ENGINEERING ===
    age_group = case_when(
      age < 30 ~ "Under_30",
      age < 40 ~ "30s",
      age < 50 ~ "40s",
      age < 60 ~ "50s",
      TRUE ~ "60_plus"
    ),
    
    training_level = case_when( # using quantiles
      weekly_km < 34 ~ "Low",
      weekly_km < 63 ~ "Medium",
      TRUE ~ "High"
    ),
    
    experience_level = case_when(
      marathons_xp == 0 ~ "First_timer",
      marathons_xp < 3 ~ "Novice",
      marathons_xp < 5 ~ "Intermediate",
      TRUE ~ "Expert"
    ),
    
    pb_tier = case_when(
      personal_best < 180 ~ "Elite",
      personal_best < 240 ~ "Fast",
      personal_best < 300 ~ "Average",
      TRUE ~ "Recreational"
    ),
    
    # === HIGH-RISK INDICATORS ===
    high_risk_age = as.numeric(age > 50),
    overexertion_risk = as.numeric(weekly_km > (age * 2)),
    inexperienced_ambitious = as.numeric(marathons_xp < 3 & personal_best < 240),
    
    # === INTERACTION TERMS ===
    age_recent_injury = age * injured_prev_mth,
    poor_support_harsh_conditions = as.numeric(hydration_stations < median(hydration_stations, na.rm = TRUE)) * 
                                   (temp_10am + humidity),
    
    # === CUMULATIVE RISK SCORE ===
    cumulative_risk_score = (injured_prev_mth * 3 + injured_prev_qtr * 2 + injured_prev_hy) * 
                           (temp_10am + humidity) / 100
  )

# Create percentile-based features within gender groups
class_data_engineered <- class_data_engineered %>%
  group_by(gender) %>%
  mutate(
    weekly_km_percentile = percent_rank(weekly_km),
    age_percentile = percent_rank(age),
    weight_percentile = percent_rank(weight),
    bmi_percentile = percent_rank(bmi),
    experience_percentile = percent_rank(marathons_xp),
    pb_percentile = percent_rank(desc(personal_best))  # desc because lower time is better
  ) %>%
  ungroup()



# Check for any missing values in new features
new_features <- setdiff(names(class_data_engineered), names(class_data))
missing_check <- sapply(class_data_engineered[new_features], function(x) sum(is.na(x)))
cat("\nMissing values in new features:\n")
print(missing_check[missing_check > 0])



model <- glm(needed_med ~ .-year, data=class_data_engineered, family=binomial())
summary(model)

lin_model <- lm(needed_med~.-year, data=class_data_engineered)
vif(lin_model)
```


lasso with f1 score
the robots code

```{r}
class_data <- data.frame(class_data_engineered)
# pick a threshold 
threshold <- 0.33

# Custom F1 score function (avoiding caret's confusionMatrix to prevent warnings)
# i actually made it fbeta
beta <- 3
f1_score <- function(actual, predicted) {
  # Ensure both are numeric 0/1
  actual <- as.numeric(actual)
  predicted <- as.numeric(predicted)
  
  # Calculate confusion matrix components manually
  tp <- sum(actual == 1 & predicted == 1)  # True Positives
  fp <- sum(actual == 0 & predicted == 1)  # False Positives
  fn <- sum(actual == 1 & predicted == 0)  # False Negatives
  
  # Calculate precision and recall
  precision <- ifelse(tp + fp == 0, 0, tp / (tp + fp))
  recall <- ifelse(tp + fn == 0, 0, tp / (tp + fn))
  
  # Calculate F1 score
  f1 <- ifelse(precision + recall == 0, 0, ((1+beta^2)*precision * recall) / (beta^2*precision + recall))
  
  return(f1)
}

# Prepare data for glmnet (same as your original model)
# Convert y to numeric (0/1) if it's a factor
y <- class_data$needed_med
if (is.factor(y)) {
  y <- as.numeric(y) - 1
}

# Split data BEFORE creating model matrix to ensure consistent factor levels
set.seed(123)
train_idx <- sample(nrow(class_data), 0.8 * nrow(class_data))
train_data <- class_data[train_idx, ]
test_data <- class_data[-train_idx, ]

# Create model matrix using training data to establish factor levels
# This ensures consistent column structure
X_train <- model.matrix(needed_med ~ . - year, data = train_data)[, -1]  # Remove intercept
y_train <- y[train_idx]

# Create test matrix using the same formula and factor levels from training
X_test <- model.matrix(needed_med ~ . - year, data = test_data)[, -1]  # Remove intercept
y_test <- y[-train_idx]

# Check dimensions match
print(paste("Training set dimensions:", nrow(X_train), "x", ncol(X_train)))
print(paste("Test set dimensions:", nrow(X_test), "x", ncol(X_test)))

# If dimensions don't match, we need to align them
if (ncol(X_train) != ncol(X_test)) {
  print("Dimension mismatch detected - aligning feature matrices...")
  
  # Get column names from both matrices
  train_cols <- colnames(X_train)
  test_cols <- colnames(X_test)
  
  # Find common columns
  common_cols <- intersect(train_cols, test_cols)
  
  # Subset both matrices to common columns
  X_train <- X_train[, common_cols, drop = FALSE]
  X_test <- X_test[, common_cols, drop = FALSE]
  
  print(paste("Aligned dimensions - Training:", nrow(X_train), "x", ncol(X_train)))
  print(paste("Aligned dimensions - Test:", nrow(X_test), "x", ncol(X_test)))
}

# Custom cross-validation with F1 score
# This is more computationally intensive but uses F1 as the selection criterion
custom_cv_lasso <- function(X, y, lambda_seq = NULL, nfolds = 10) {
  if (is.null(lambda_seq)) {
    # Use glmnet's default lambda sequence
    temp_fit <- glmnet(X, y, alpha = 1, family = "binomial")
    lambda_seq <- temp_fit$lambda
  }
  
  # Create folds
  folds <- createFolds(y, k = nfolds, list = TRUE)
  
  # Initialize results matrix
  f1_scores <- matrix(NA, nrow = nfolds, ncol = length(lambda_seq))
  
  # Cross-validation loop
  for (fold in 1:nfolds) {
    # Split data
    train_idx <- unlist(folds[-fold])
    val_idx <- folds[[fold]]
    
    X_fold_train <- X[train_idx, ]
    y_fold_train <- y[train_idx]
    X_fold_val <- X[val_idx, ]
    y_fold_val <- y[val_idx]
    
    # Fit model for all lambdas
    fold_model <- glmnet(X_fold_train, y_fold_train, 
                        alpha = 1, family = "binomial",
                        lambda = lambda_seq)
    
    # Predict and calculate F1 for each lambda
    for (j in 1:length(lambda_seq)) {
      pred_prob <- predict(fold_model, newx = X_fold_val, 
                          s = lambda_seq[j], type = "response")
      pred_class <- ifelse(pred_prob > threshold, 1, 0)
      
      # Calculate F1 score
      f1_scores[fold, j] <- f1_score(y_fold_val, pred_class)
    }
  }
  
  # Calculate mean F1 score for each lambda
  mean_f1 <- colMeans(f1_scores, na.rm = TRUE)
  
  # Find lambda with highest mean F1 score
  best_lambda_idx <- which.max(mean_f1)
  best_lambda <- lambda_seq[best_lambda_idx]
  best_f1 <- mean_f1[best_lambda_idx]
  
  return(list(
    lambda = lambda_seq,
    mean_f1 = mean_f1,
    best_lambda = best_lambda,
    best_f1 = best_f1,
    f1_scores = f1_scores
  ))
}

cv_f1_results <- custom_cv_lasso(X_train, y_train, nfolds = 10)

print(paste("Best lambda (F1-based):", round(cv_f1_results$best_lambda, 6)))
print(paste("Best F1 score:", round(cv_f1_results$best_f1, 4)))

# Fit final model with F1-optimized lambda
lasso_model <- glmnet(X_train, y_train, 
                      alpha = 1, 
                      family = "binomial",
                      lambda = cv_f1_results$best_lambda)

# Make predictions on test set
pred_prob <- predict(lasso_model, newx = X_test, type = "response")
pred_class <- ifelse(pred_prob > threshold, 1, 0)

# Evaluate model
f1_test <- f1_score(y_test, pred_class)

print("\n=== Model Performance ===")
print(paste("Test F1 Score:", round(f1_test, 4)))
table(pred_class, actual=y_test)

# Show selected features (non-zero coefficients)
coef_model <- coef(lasso_model)
selected_features <- rownames(coef_model)[which(coef_model != 0)]

print(paste("\nFeatures selected:", length(selected_features) - 1))  # -1 for intercept

# Display the features
print("\nSelected features:")
print(selected_features)

# Plot CV results and coefficient paths
par(mfrow = c(2, 2))

# 1. F1 CV plot
plot(log(cv_f1_results$lambda), cv_f1_results$mean_f1, 
     xlab = "log(Lambda)", ylab = "Mean F1 Score",
     main = "Cross-Validation (F1 Score)")
abline(v = log(cv_f1_results$best_lambda), col = "red", lty = 2)

# 2. Coefficient path plot - shows all parameters shrinking
# Fit model across full lambda sequence for plotting
full_lasso <- glmnet(X_train, y_train, alpha = 1, family = "binomial")
plot(full_lasso, xvar = "lambda", label = TRUE)
title("LASSO Coefficient Paths")
abline(v = log(cv_f1_results$best_lambda), col = "red", lty = 2, lwd = 2)
legend("topright", legend = "F1-optimal λ", col = "red", lty = 2, lwd = 2)

# 3. Alternative coefficient path plot by L1 norm
plot(full_lasso, xvar = "norm", label = TRUE)
title("Coefficient Paths by L1 Norm")

# Reset plotting parameters
par(mfrow = c(1, 1))

# Additional detailed coefficient path plot
# Create a standalone detailed plot
plot(full_lasso, xvar = "lambda", label = TRUE, cex.axis = 1.2, cex.lab = 1.2)
title("LASSO Regularization Path\n(Coefficient Shrinkage vs Log Lambda)", cex.main = 1.3)
abline(v = log(cv_f1_results$best_lambda), col = "red", lty = 2, lwd = 2)
legend("topright", 
       legend = "F1-optimal λ", 
       col = "red", 
       lty = 2, lwd = 2,
       cex = 1.1)

# Print coefficient information at different lambda values
print("\n=== Coefficient Information ===")
print("Number of non-zero coefficients at different lambda values:")
print(paste("At lambda =", round(max(full_lasso$lambda), 6), ":", full_lasso$df[1]))
print(paste("At F1-optimal lambda =", round(cv_f1_results$best_lambda, 6), ":", sum(coef(full_lasso, s = cv_f1_results$best_lambda) != 0) - 1))
print(paste("At lambda =", round(min(full_lasso$lambda), 6), ":", max(full_lasso$df)))

# Return the final model
final_model <- lasso_model
print("\nFinal model optimized for F1 score")
print(paste("Final lambda:", round(cv_f1_results$best_lambda, 6)))
print(paste("Number of selected features:", length(selected_features) - 1))

```
